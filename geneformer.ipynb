{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oOEeg11MRszN"
      },
      "outputs": [],
      "source": [
        "#!git clone https://huggingface.co/ctheodoris/Geneformer\n",
        "#!pip install loompy\n",
        "import os\n",
        "GPU_NUMBER = [0]\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([str(s) for s in GPU_NUMBER])\n",
        "os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
        "from collections import Counter\n",
        "import datetime\n",
        "import pickle\n",
        "import subprocess\n",
        "import seaborn as sns; sns.set()\n",
        "from datasets import load_from_disk\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from transformers import BertForSequenceClassification\n",
        "from transformers import BertModel\n",
        "from transformers import Trainer\n",
        "from transformers.training_args import TrainingArguments\n",
        "\n",
        "from geneformer import DataCollatorForCellClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkKa6PNRSIJy",
        "outputId": "1b6439f6-f076-42d0-bdba-932b37a68271"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['input_ids', 'cell_type', 'length'],\n",
            "    num_rows: 60153\n",
            "})\n",
            "Dataset({\n",
            "    features: ['input_ids', 'cell_type', 'length'],\n",
            "    num_rows: 16653\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "train_dataset=load_from_disk(\"train.dataset\")\n",
        "test_dataset = load_from_disk(\"test.dataset\")\n",
        "\n",
        "print(train_dataset)\n",
        "print(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDvAXYaVWMHT",
        "outputId": "cb26f725-5b95-4ff3-8b22-4c298d3c2788"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /content/train.dataset/cache-ee281e0d78d79a49.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /content/test.dataset/cache-679a8ce8464d095e.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /content/train.dataset/cache-78889b7b6e2b8ce1.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /content/test.dataset/cache-cf3e4d227bcea660.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /content/test.dataset/cache-29942f3d0c0ffbfd.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'nasal mucosa goblet cell': 0, 'club cell': 1, 'respiratory basal cell': 2, 'respiratory hillock cell': 3, 'multi-ciliated epithelial cell': 4, 'ciliated columnar cell of tracheobronchial tree': 5, 'CD1c-positive myeloid dendritic cell': 6, 'bronchus fibroblast of lung': 7, 'CD8-positive, alpha-beta T cell': 8, 'mast cell': 9, 'B cell': 10, 'acinar cell': 11, 'plasmacytoid dendritic cell': 12, 'classical monocyte': 13, 'CD4-positive, alpha-beta T cell': 14, 'natural killer cell': 15, 'elicited macrophage': 16, 'ionocyte': 17, 'alveolar type 2 fibroblast cell': 18, 'alveolar macrophage': 19, 'dendritic cell': 20, 'brush cell of trachebronchial tree': 21, 'mucus secreting cell': 22, 'non-classical monocyte': 23, 'lung macrophage': 24, 'conventional dendritic cell': 25, 'tracheobronchial goblet cell': 26, 'vein endothelial cell': 27, 'tracheobronchial smooth muscle cell': 28, 'tracheobronchial serous cell': 29, 'bronchial goblet cell': 30, 'epithelial cell of lower respiratory tract': 31, 'T cell': 32, 'plasma cell': 33}]\n"
          ]
        }
      ],
      "source": [
        "dataset_list = []\n",
        "evalset_list = []\n",
        "organ_list = []\n",
        "target_dict_list = []\n",
        "\n",
        "trainset_organ_shuffled = train_dataset.shuffle(seed=42)\n",
        "trainset_organ_shuffled = trainset_organ_shuffled.rename_column(\"cell_type\",\"label\")\n",
        "\n",
        "testset_organ_shuffled = test_dataset.shuffle(seed=42)\n",
        "testset_organ_shuffled = testset_organ_shuffled.rename_column(\"cell_type\",\"label\")\n",
        "\n",
        "\n",
        "\n",
        "target_names = list(Counter(trainset_organ_shuffled[\"label\"]).keys())\n",
        "target_name_id_dict = dict(zip(target_names,[i for i in range(len(target_names))]))\n",
        "target_dict_list += [target_name_id_dict]\n",
        "print(target_dict_list)\n",
        "\n",
        "\n",
        "def classes_to_ids(example):\n",
        "    example[\"label\"] = target_name_id_dict[example[\"label\"]]\n",
        "    return example\n",
        "labeled_train_split = trainset_organ_shuffled.map(classes_to_ids, num_proc=1)\n",
        "labeled_eval_split = testset_organ_shuffled.map(classes_to_ids, num_proc=1)\n",
        "\n",
        "\n",
        "\n",
        "labeled_train_split = labeled_train_split.select([i for i in range(0, round(len(labeled_train_split)*0.1))])\n",
        "labeled_eval_split = labeled_eval_split.select([i for i in range(0, round(len(labeled_eval_split)*0.1))])\n",
        "\n",
        "\n",
        "trained_labels = list(Counter(labeled_train_split[\"label\"]).keys())\n",
        "def if_trained_label(example):\n",
        "    return example[\"label\"] in trained_labels\n",
        "labeled_eval_split_subset = labeled_eval_split.filter(if_trained_label, num_proc=1)\n",
        "\n",
        "dataset_list += [labeled_train_split]\n",
        "evalset_list += [labeled_eval_split_subset]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQ2jXan0WURR",
        "outputId": "a1926c42-dece-4e0b-be8d-dfb714bc0508"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': [20336, 17592, 11469, 4542, 8170, 12757, 5982, 13335, 7557, 4371, 2507, 8337, 6620, 17310, 17984, 9744, 7450, 14142, 11867, 8028, 2368, 3390, 9995, 10588, 14313, 12510, 2920, 7611, 355, 7544, 4702, 7114, 3964, 16137, 8584, 74, 861, 3076, 5657, 9957, 15056, 2976, 14091, 3756, 13647, 13288, 7140, 6286, 10201, 964, 7570, 9043, 13127, 5056, 16419, 1341, 11485, 2239, 1651, 1442, 4378, 10907, 11753, 1073, 12752, 2448, 5848, 13848, 6900, 3655, 5927, 10218, 13818, 8194, 6395, 14496, 12244, 5993, 1976, 10198, 332, 9260, 3313, 19666, 10449, 6144, 12261, 5023, 7026, 12492, 940, 10068, 8341, 17906, 3972, 2894, 7209, 8135, 4576, 20099, 6124, 3063, 7263, 4990, 4154, 3057, 4487, 3046, 3493, 13856, 1922, 15572, 2555, 4857, 1729, 4341, 2723, 1148, 12541, 5172, 5235, 745, 1802, 4732, 933, 1206, 3271, 17101, 6633, 1765, 12756, 14172, 936, 2919, 15587, 4251, 893, 18754, 1418, 10336, 17973, 8792, 520, 8288, 4533, 16378, 10800, 8939, 3092, 4557, 4516, 6037, 2638, 3603, 3990, 14744, 18830, 6625, 4804, 23941, 3918, 17302, 14116, 12790, 3130, 1702, 15784, 2791, 5748, 5919, 16734, 14157, 18369, 8013, 5602, 1146, 4951, 18590, 3724, 8198, 727, 9049, 7947, 12889, 8039, 934, 12184, 4209, 776, 1827, 12926, 16955, 20440, 3866, 2546, 13413, 15255, 6317, 16346, 11314, 2364, 5569, 9009, 5556, 11745, 3218, 10509, 5826, 362, 8462, 4703, 6357, 6487, 3654, 6713, 6383, 4065, 17321, 4509, 7457, 4350, 1820, 10548, 12529, 7186, 1351, 17987, 5613, 912, 2263, 5791, 2104, 7143, 16165, 4825, 13135, 5583, 9746, 8386, 3826, 7193, 3652, 2066, 6652, 5716, 5261, 1952, 11406, 16737, 3942, 1965, 4919, 1830, 12252, 13865, 7295, 1311, 14964, 14341, 2526, 10941, 15602, 4189, 14947, 1626, 5785, 2804, 11143, 5647, 9220, 16239, 2551, 6960, 2814, 6944, 12716, 6179, 16337, 11347, 1803, 19925, 7053, 11543, 3917, 6376, 955, 10877, 3835, 3560, 3259, 12076, 8215, 22668, 695, 12428, 2928, 2400, 8229, 10517, 5354, 1984, 16602, 3554, 16445, 10837, 9300, 12929, 7055, 16423, 18489, 3312, 17626, 16979, 17236, 16572, 16906, 12072, 4547, 3537, 3906, 1823, 12182, 6561, 4275, 10798, 18591, 396, 14983, 771, 15299, 10511, 1614, 7345, 12172, 7606, 9025, 20499, 856, 1720, 16980, 15993, 17071, 3502, 13810, 2257, 8256, 16797, 3792, 9142, 3482, 19999, 17208, 8008, 7379, 4387, 4037, 15803, 5752, 6681, 14125, 5433, 512, 8565, 8659, 17200, 5212, 7502, 8231, 17326, 12030, 11319, 19823, 17247, 12735, 8208, 17303, 3664, 11842, 8919, 17295, 9945, 19901, 17905], 'label': 3, 'length': 383}\n"
          ]
        }
      ],
      "source": [
        "print(dataset_list[0][500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xGnvcl9FWaDk"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    # calculate accuracy and macro f1 using sklearn's function\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    macro_f1 = f1_score(labels, preds, average='macro')\n",
        "    return {\n",
        "      'accuracy': acc,\n",
        "      'macro_f1': macro_f1\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QeUOwkvBWcmR"
      },
      "outputs": [],
      "source": [
        "# set model parameters\n",
        "# max input size\n",
        "max_input_size = 2 ** 11  # 2048\n",
        "\n",
        "# set training parameters\n",
        "# max learning rate\n",
        "max_lr = 5e-5\n",
        "# how many pretrained layers to freeze\n",
        "freeze_layers = 0\n",
        "# number gpus\n",
        "num_gpus = 1\n",
        "# number cpu cores\n",
        "num_proc = 1\n",
        "# batch size for training and eval\n",
        "geneformer_batch_size = 12\n",
        "# learning schedule\n",
        "lr_schedule_fn = \"linear\"\n",
        "# warmup steps\n",
        "warmup_steps = 500\n",
        "# number of epochs\n",
        "epochs = 5\n",
        "# optimizer\n",
        "optimizer = \"adamw\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjR0fgQdZDDj",
        "outputId": "34f54250-8417-4f21-d5ef-04d99775d547"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (16.0.6)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.20.3\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "10TkFmLTpUth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "uPAhTaDKWjGk",
        "outputId": "cea1de48-e45f-4861-af2b-a10f13e31f63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at model were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at model and are newly initialized: ['bert.pooler.dense.bias', 'classifier.weight', 'classifier.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at model were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at model and are newly initialized: ['bert.pooler.dense.bias', 'classifier.weight', 'classifier.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/content/geneformer/collator_for_classification.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2510' max='2510' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2510/2510 12:31, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Macro F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.750300</td>\n",
              "      <td>2.611466</td>\n",
              "      <td>0.544518</td>\n",
              "      <td>0.071650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.777700</td>\n",
              "      <td>1.695578</td>\n",
              "      <td>0.506965</td>\n",
              "      <td>0.065764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.579700</td>\n",
              "      <td>1.459867</td>\n",
              "      <td>0.449425</td>\n",
              "      <td>0.052605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.485300</td>\n",
              "      <td>1.390865</td>\n",
              "      <td>0.460933</td>\n",
              "      <td>0.055215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.457300</td>\n",
              "      <td>1.372756</td>\n",
              "      <td>0.473652</td>\n",
              "      <td>0.058126</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/geneformer/collator_for_classification.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "/content/geneformer/collator_for_classification.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "/content/geneformer/collator_for_classification.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "/content/geneformer/collator_for_classification.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "/content/geneformer/collator_for_classification.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ],
      "source": [
        "organ_trainset = dataset_list[0]\n",
        "organ_evalset = evalset_list[0]\n",
        "organ_label_dict = target_dict_list[0]\n",
        "print(len(organ_trainset[:1000]))\n",
        "\n",
        "    # set logging steps\n",
        "logging_steps = round(len(organ_trainset)/geneformer_batch_size/10)\n",
        "\n",
        "    # reload pretrained model\n",
        "model1 = BertForSequenceClassification.from_pretrained(\"model\",\n",
        "                                                    num_labels=len(organ_label_dict.keys()),\n",
        "                                                    output_attentions = False,\n",
        "                                                    output_hidden_states = False).to(\"cuda\")\n",
        "\n",
        "model2 = BertForSequenceClassification.from_pretrained(\"model\",\n",
        "                                                    num_labels=len(organ_label_dict.keys()),\n",
        "                                                    output_attentions = False,\n",
        "                                                    output_hidden_states = False).to(\"cuda\")\n",
        "\n",
        "\n",
        "for param in model1.bert.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "    # define output directory path\n",
        "current_date = datetime.datetime.now()\n",
        "datestamp = f\"{str(current_date.year)[-2:]}{current_date.month:02d}{current_date.day:02d}\"\n",
        "output_dir = f\"/path/to/models/{datestamp}_L{max_input_size}_B{geneformer_batch_size}_LR{max_lr}_LS{lr_schedule_fn}_WU{warmup_steps}_E{epochs}_O{optimizer}_F{freeze_layers}/\"\n",
        "\n",
        "    # ensure not overwriting previously saved model\n",
        "saved_model_test = os.path.join(output_dir, f\"pytorch_model.bin\")\n",
        "if os.path.isfile(saved_model_test) == True:\n",
        "    raise Exception(\"Model already saved to this directory.\")\n",
        "\n",
        "    # make output directory\n",
        "subprocess.call(f'mkdir {output_dir}', shell=True)\n",
        "\n",
        "    # set training arguments\n",
        "training_args = {\n",
        "    \"learning_rate\": max_lr,\n",
        "    \"do_train\": True,\n",
        "    \"do_eval\": True,\n",
        "    \"evaluation_strategy\": \"epoch\",\n",
        "    \"save_strategy\": \"epoch\",\n",
        "    \"logging_steps\": logging_steps,\n",
        "    \"group_by_length\": True,\n",
        "    \"length_column_name\": \"length\",\n",
        "    \"disable_tqdm\": False,\n",
        "    \"lr_scheduler_type\": lr_schedule_fn,\n",
        "    \"warmup_steps\": warmup_steps,\n",
        "    \"weight_decay\": 0.001,\n",
        "    \"per_device_train_batch_size\": geneformer_batch_size,\n",
        "    \"per_device_eval_batch_size\": geneformer_batch_size,\n",
        "    \"num_train_epochs\": epochs,\n",
        "    \"load_best_model_at_end\": True,\n",
        "    \"output_dir\": output_dir,\n",
        "}\n",
        "\n",
        "training_args_init = TrainingArguments(**training_args)\n",
        "\n",
        "    # create the trainer\n",
        "trainer = Trainer(\n",
        "    model=model1,\n",
        "    args=training_args_init,\n",
        "    data_collator=DataCollatorForCellClassification(),\n",
        "    train_dataset=organ_trainset,\n",
        "    eval_dataset=organ_evalset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "    # train the cell type classifier\n",
        "trainer.train()\n",
        "predictions = trainer.predict(organ_evalset)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args2 = {\n",
        "    \"learning_rate\": max_lr,\n",
        "    \"do_train\": True,\n",
        "    \"do_eval\": True,\n",
        "    \"evaluation_strategy\": \"epoch\",\n",
        "    \"save_strategy\": \"epoch\",\n",
        "    \"logging_steps\": logging_steps,\n",
        "    \"group_by_length\": True,\n",
        "    \"length_column_name\": \"length\",\n",
        "    \"disable_tqdm\": False,\n",
        "    \"lr_scheduler_type\": lr_schedule_fn,\n",
        "    \"warmup_steps\": warmup_steps,\n",
        "    \"weight_decay\": 0.001,\n",
        "    \"per_device_train_batch_size\": geneformer_batch_size,\n",
        "    \"per_device_eval_batch_size\": geneformer_batch_size,\n",
        "    \"num_train_epochs\": 1,\n",
        "    \"load_best_model_at_end\": True,\n",
        "    \"output_dir\": output_dir,\n",
        "}\n",
        "\n",
        "training_args_init2 = TrainingArguments(**training_args2)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model2,\n",
        "    args=training_args_init2,\n",
        "    data_collator=DataCollatorForCellClassification(),\n",
        "    train_dataset=organ_trainset,\n",
        "    eval_dataset=organ_evalset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "    # train the cell type classifier\n",
        "trainer.train()\n",
        "\n",
        "for param in model2.bert.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model2,\n",
        "    args=training_args_init,\n",
        "    data_collator=DataCollatorForCellClassification(),\n",
        "    train_dataset=organ_trainset,\n",
        "    eval_dataset=organ_evalset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "    # train the cell type classifier\n",
        "trainer.train()\n",
        "predictions = trainer.predict(organ_evalset)"
      ],
      "metadata": {
        "id": "wPDkdX61r5Wj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "outputId": "7f3da047-46fb-41a2-ae4b-872a4761db36"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='502' max='502' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [502/502 04:55, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Macro F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.858800</td>\n",
              "      <td>0.772802</td>\n",
              "      <td>0.775893</td>\n",
              "      <td>0.178279</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/content/geneformer/collator_for_classification.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2510' max='2510' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2510/2510 12:32, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Macro F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.690600</td>\n",
              "      <td>0.723827</td>\n",
              "      <td>0.782556</td>\n",
              "      <td>0.186841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.609600</td>\n",
              "      <td>0.702741</td>\n",
              "      <td>0.791641</td>\n",
              "      <td>0.215139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.565500</td>\n",
              "      <td>0.699235</td>\n",
              "      <td>0.794064</td>\n",
              "      <td>0.233322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.590600</td>\n",
              "      <td>0.690764</td>\n",
              "      <td>0.797698</td>\n",
              "      <td>0.233771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.596600</td>\n",
              "      <td>0.693066</td>\n",
              "      <td>0.796487</td>\n",
              "      <td>0.233590</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/geneformer/collator_for_classification.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "/content/geneformer/collator_for_classification.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "/content/geneformer/collator_for_classification.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "/content/geneformer/collator_for_classification.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
            "/content/geneformer/collator_for_classification.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOHGD5jrv03X"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from geneformer import DataCollatorForCellClassification\n",
        "class DataCollatorForCellClassification(DataCollatorForGeneClassification):\n",
        "\n",
        "    class_type = \"cell\"\n",
        "\n",
        "    def _prepare_batch(self, features):\n",
        "\n",
        "        batch = super()._prepare_batch(features)\n",
        "\n",
        "        # Special handling for labels.\n",
        "        # Ensure that tensor is created with the correct type\n",
        "        # (it should be automatically the case, but let's make sure of it.)\n",
        "        first = features[0]\n",
        "        if \"label\" in first and first[\"label\"] is not None:\n",
        "            label = first[\"label\"].item() if isinstance(first[\"label\"], torch.Tensor) else first[\"label\"]\n",
        "            dtype = torch.long if isinstance(label, int) else torch.float\n",
        "\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 893
        },
        "id": "5AwK8xsDmNgE",
        "outputId": "2b5a21f1-0da1-4808-add9-11454e26c52d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at model were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at model and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/content/geneformer/collator_for_classification.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BertModel(\n",
            "  (embeddings): BertEmbeddings(\n",
            "    (word_embeddings): Embedding(25426, 256, padding_idx=0)\n",
            "    (position_embeddings): Embedding(2048, 256)\n",
            "    (token_type_embeddings): Embedding(2, 256)\n",
            "    (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
            "    (dropout): Dropout(p=0.02, inplace=False)\n",
            "  )\n",
            "  (encoder): BertEncoder(\n",
            "    (layer): ModuleList(\n",
            "      (0-5): 6 x BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (dropout): Dropout(p=0.02, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.02, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=256, out_features=512, bias=True)\n",
            "          (intermediate_act_fn): ReLU()\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=512, out_features=256, bias=True)\n",
            "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.02, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (pooler): BertPooler(\n",
            "    (dense): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (activation): Tanh()\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "raw_model = BertModel.from_pretrained(\"model\",\n",
        "                                                    output_attentions = False,\n",
        "                                                    output_hidden_states = False).to(\"cuda\")\n",
        "print(raw_model)\n",
        "trainer = Trainer(\n",
        "    model=raw_model,\n",
        "    args=training_args_init,\n",
        "    data_collator=DataCollatorForCellClassification(),\n",
        "    train_dataset=organ_trainset,\n",
        "    eval_dataset=organ_evalset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "result = trainer.predict(organ_evalset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8asTWjpJoA1e",
        "outputId": "39194ab5-8e53-4158-a746-c710c43a71f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 0.5448414   0.7326044   2.515304   ... -2.715903   -0.8966244\n",
            "  -1.6133952 ]\n",
            " [ 0.9634073   0.5889873   2.4637346  ... -2.023886    1.0938008\n",
            "  -1.0942581 ]\n",
            " [ 0.35646796  1.2334421   2.8845596  ... -2.2046096  -0.3802756\n",
            "  -1.8154938 ]\n",
            " ...\n",
            " [-1.3833662  -2.842306    0.16176957 ... -0.81125003 -0.18596315\n",
            "   0.39208692]\n",
            " [-1.3778925  -3.075493    0.3745582  ... -0.33856562  0.06206445\n",
            "   0.68185574]\n",
            " [-1.4362442  -2.7504442   0.8110077  ... -0.30803168  0.83822453\n",
            "   0.15221353]]\n"
          ]
        }
      ],
      "source": [
        "print(result[0][0][0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}